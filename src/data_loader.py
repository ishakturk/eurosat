"""
EuroSAT Veri YÃ¼kleme ModÃ¼lÃ¼
===========================
Bu modÃ¼l veri setinin indirilmesi, Ã¶n iÅŸleme ve DataLoader oluÅŸturma iÅŸlemlerini iÃ§erir.
"""

import os
import torch
import zipfile
import urllib.request
from torch.utils.data import DataLoader, random_split
from torchvision import datasets, transforms
from PIL import Image


# ==================== SABITLER ====================
# ImageNet normalizasyon deÄŸerleri (Transfer Learning iÃ§in standart)
IMAGENET_MEAN = [0.485, 0.456, 0.406]
IMAGENET_STD = [0.229, 0.224, 0.225]

# EuroSAT sÄ±nÄ±flarÄ±
CLASS_NAMES = [
    'AnnualCrop', 'Forest', 'HerbaceousVegetation', 'Highway', 'Industrial',
    'Pasture', 'PermanentCrop', 'Residential', 'River', 'SeaLake'
]

NUM_CLASSES = 10


# ==================== VERÄ° DÃ–NÃœÅÃœM FONKSÄ°YONLARI ====================
def get_train_transforms(img_size=224):
    """
    EÄŸitim iÃ§in veri artÄ±rma (data augmentation) dÃ¶nÃ¼ÅŸÃ¼mleri.

    Args:
        img_size: Hedef gÃ¶rÃ¼ntÃ¼ boyutu (varsayÄ±lan: 224x224)

    Returns:
        torchvision.transforms.Compose: DÃ¶nÃ¼ÅŸÃ¼m pipeline'Ä±
    """
    return transforms.Compose([
        transforms.Resize((img_size, img_size)),
        transforms.RandomHorizontalFlip(p=0.5),
        transforms.RandomVerticalFlip(p=0.5),
        transforms.RandomRotation(degrees=15),
        transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2),
        transforms.ToTensor(),
        transforms.Normalize(mean=IMAGENET_MEAN, std=IMAGENET_STD)
    ])


def get_test_transforms(img_size=224):
    """
    Test/Validation iÃ§in dÃ¶nÃ¼ÅŸÃ¼mler (augmentation yok).

    Args:
        img_size: Hedef gÃ¶rÃ¼ntÃ¼ boyutu (varsayÄ±lan: 224x224)

    Returns:
        torchvision.transforms.Compose: DÃ¶nÃ¼ÅŸÃ¼m pipeline'Ä±
    """
    return transforms.Compose([
        transforms.Resize((img_size, img_size)),
        transforms.ToTensor(),
        transforms.Normalize(mean=IMAGENET_MEAN, std=IMAGENET_STD)
    ])


# ==================== VERÄ° SETÄ° Ä°NDÄ°RME ====================
def download_eurosat(data_dir='./data', extract_dir='./data'):
    """
    EuroSAT RGB veri setini indirir ve Ã§Ä±karÄ±r.

    Args:
        data_dir: Ä°ndirme dizini
        extract_dir: Ã‡Ä±karma dizini

    Returns:
        str: Veri seti klasÃ¶r yolu
    """
    url = "http://madm.dfki.de/files/sentinel/EuroSAT.zip"
    zip_path = os.path.join(data_dir, "EuroSAT.zip")
    dataset_path = os.path.join(extract_dir, "2750")  # RGB versiyonu

    # KlasÃ¶r oluÅŸtur
    os.makedirs(data_dir, exist_ok=True)

    # EÄŸer zaten varsa indirme
    if os.path.exists(dataset_path):
        print(f"âœ“ Veri seti zaten mevcut: {dataset_path}")
        return dataset_path

    # Ä°ndir
    print(f"â¬‡ Veri seti indiriliyor: {url}")
    urllib.request.urlretrieve(url, zip_path)
    print("âœ“ Ä°ndirme tamamlandÄ±!")

    # Zip'i Ã§Ä±kar
    print("ğŸ“¦ Zip dosyasÄ± Ã§Ä±karÄ±lÄ±yor...")
    with zipfile.ZipFile(zip_path, 'r') as zip_ref:
        zip_ref.extractall(extract_dir)
    print("âœ“ Ã‡Ä±karma tamamlandÄ±!")

    # Zip dosyasÄ±nÄ± sil (opsiyonel)
    os.remove(zip_path)

    return dataset_path


# ==================== DATALOADER OLUÅTURMA ====================
def create_dataloaders(data_path, batch_size=32, train_ratio=0.8, val_ratio=0.1,
                       num_workers=2, img_size=224, seed=42):
    """
    Train, Validation ve Test DataLoader'larÄ± oluÅŸturur.

    Args:
        data_path: Veri seti klasÃ¶r yolu
        batch_size: Batch boyutu
        train_ratio: EÄŸitim oranÄ± (varsayÄ±lan: %80)
        val_ratio: Validation oranÄ± (varsayÄ±lan: %10)
        num_workers: DataLoader worker sayÄ±sÄ±
        img_size: GÃ¶rÃ¼ntÃ¼ boyutu
        seed: Random seed (tekrarlanabilirlik iÃ§in)

    Returns:
        tuple: (train_loader, val_loader, test_loader, dataset_info)
    """
    # Seed ayarla
    torch.manual_seed(seed)

    # Transform'larÄ± al
    train_transform = get_train_transforms(img_size)
    test_transform = get_test_transforms(img_size)

    # Tam veri setini yÃ¼kle (transform olmadan bÃ¶lmek iÃ§in)
    full_dataset = datasets.ImageFolder(root=data_path)

    # Veri setini bÃ¶l
    total_size = len(full_dataset)
    train_size = int(train_ratio * total_size)
    val_size = int(val_ratio * total_size)
    test_size = total_size - train_size - val_size

    train_dataset, val_dataset, test_dataset = random_split(
        full_dataset,
        [train_size, val_size, test_size],
        generator=torch.Generator().manual_seed(seed)
    )

    # Her bir subset'e uygun transform uygula
    # Not: Subset kullanÄ±rken transform'u wrapper class ile uyguluyoruz
    train_dataset = TransformSubset(train_dataset, train_transform)
    val_dataset = TransformSubset(val_dataset, test_transform)
    test_dataset = TransformSubset(test_dataset, test_transform)

    # DataLoader'larÄ± oluÅŸtur
    train_loader = DataLoader(
        train_dataset,
        batch_size=batch_size,
        shuffle=True,
        num_workers=num_workers,
        pin_memory=True
    )

    val_loader = DataLoader(
        val_dataset,
        batch_size=batch_size,
        shuffle=False,
        num_workers=num_workers,
        pin_memory=True
    )

    test_loader = DataLoader(
        test_dataset,
        batch_size=batch_size,
        shuffle=False,
        num_workers=num_workers,
        pin_memory=True
    )

    # Dataset bilgisi
    dataset_info = {
        'total_samples': total_size,
        'train_samples': train_size,
        'val_samples': val_size,
        'test_samples': test_size,
        'num_classes': NUM_CLASSES,
        'class_names': CLASS_NAMES,
        'img_size': img_size
    }

    print("\n" + "="*50)
    print("ğŸ“Š VERÄ° SETÄ° BÄ°LGÄ°LERÄ°")
    print("="*50)
    print(f"Toplam gÃ¶rÃ¼ntÃ¼ sayÄ±sÄ±: {total_size:,}")
    print(f"EÄŸitim seti: {train_size:,} ({train_ratio*100:.0f}%)")
    print(f"DoÄŸrulama seti: {val_size:,} ({val_ratio*100:.0f}%)")
    print(f"Test seti: {test_size:,} ({(1-train_ratio-val_ratio)*100:.0f}%)")
    print(f"SÄ±nÄ±f sayÄ±sÄ±: {NUM_CLASSES}")
    print(f"GÃ¶rÃ¼ntÃ¼ boyutu: {img_size}x{img_size}")
    print("="*50 + "\n")

    return train_loader, val_loader, test_loader, dataset_info


class TransformSubset(torch.utils.data.Dataset):
    """
    Subset'e transform uygulayan wrapper sÄ±nÄ±f.
    random_split sonrasÄ±nda her bir bÃ¶lÃ¼me farklÄ± transform uygulamak iÃ§in kullanÄ±lÄ±r.
    """
    def __init__(self, subset, transform=None):
        self.subset = subset
        self.transform = transform

    def __getitem__(self, idx):
        img, label = self.subset[idx]
        if self.transform:
            img = self.transform(img)
        return img, label

    def __len__(self):
        return len(self.subset)


# ==================== GÃ–RÃœNTÃœ Ã–N Ä°ZLEME ====================
def denormalize(tensor, mean=IMAGENET_MEAN, std=IMAGENET_STD):
    """
    Normalize edilmiÅŸ tensÃ¶rÃ¼ orijinal haline dÃ¶ndÃ¼rÃ¼r (gÃ¶rselleÅŸtirme iÃ§in).

    Args:
        tensor: Normalize edilmiÅŸ gÃ¶rÃ¼ntÃ¼ tensÃ¶rÃ¼
        mean: Normalizasyon ortalamasÄ±
        std: Normalizasyon standart sapmasÄ±

    Returns:
        tensor: Denormalize edilmiÅŸ tensÃ¶r
    """
    mean = torch.tensor(mean).view(3, 1, 1)
    std = torch.tensor(std).view(3, 1, 1)
    return tensor * std + mean


# ==================== TEST ====================
if __name__ == "__main__":
    # Test iÃ§in veri setini indir ve DataLoader oluÅŸtur
    print("EuroSAT Data Loader Test")
    print("-" * 40)

    # Veri setini indir
    data_path = download_eurosat(data_dir='./data', extract_dir='./data')

    # DataLoader'larÄ± oluÅŸtur
    train_loader, val_loader, test_loader, info = create_dataloaders(
        data_path=data_path,
        batch_size=32,
        img_size=224
    )

    # Bir batch kontrol et
    images, labels = next(iter(train_loader))
    print(f"\nBatch boyutu: {images.shape}")
    print(f"Label boyutu: {labels.shape}")
    print(f"Ä°lk 5 label: {[CLASS_NAMES[l] for l in labels[:5].tolist()]}")

